{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import warnings\n",
    "from typing import List, Tuple\n",
    "from urllib import request\n",
    "import pandas as pd\n",
    "from pandas import DataFrame as df\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from requests import get\n",
    "from requests.exceptions import HTTPError\n",
    "from contextlib import closing\n",
    "from bs4 import BeautifulSoup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Source(object):\n",
    "    name: str\n",
    "    site_url: str\n",
    "    country: str\n",
    "    bias: str\n",
    "    factual: str\n",
    "    press_freedom: str\n",
    "    media_type: str\n",
    "    popularity: str\n",
    "    MBFC_credibility: str\n",
    "    image_url: str\n",
    "    page_url: str\n",
    "    biaser: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_website_name(url):\n",
    "    website_name = url.split('/')[2]\n",
    "    return website_name\n",
    "    \n",
    "def is_good_response(resp):\n",
    "    \"\"\"\n",
    "    Returns True if the response seems to be HTML, False otherwise.\n",
    "    \"\"\"\n",
    "    content_type = resp.headers['Content-Type'].lower()\n",
    "    return (resp.status_code == 200\n",
    "            and content_type is not None\n",
    "            and content_type.find('html') > -1)\n",
    "\n",
    "def simple_get(url):\n",
    "    \"\"\"\n",
    "    Attempts to get the content at `url` by making an HTTP GET request.\n",
    "    If the content-type of response is some kind of HTML/XML, return the\n",
    "    text content, otherwise raise Exception.\n",
    "    \"\"\"\n",
    "    with closing(get(url, stream=True)) as resp:\n",
    "        if is_good_response(resp):\n",
    "            return resp.content\n",
    "        else:\n",
    "            resp.raise_for_status()\n",
    "\n",
    "def get_pages(sources) -> List[str]:\n",
    "    \"\"\"\n",
    "    Gets all known media pages from the category pages specified in the function.\n",
    "    :return: The media pages to be scraped.\n",
    "    \"\"\"\n",
    "    \n",
    "    pages: List[str] = []\n",
    "\n",
    "    for source in sources:\n",
    "        print('# # # # # # # # # # # # # #')\n",
    "        print('Gathering pages in this category/site:')\n",
    "        print(source)\n",
    "        raw_html = simple_get(source)\n",
    "        bs = BeautifulSoup(raw_html, 'html.parser')\n",
    "        links = bs.find_all('td')\n",
    "        for a in links:\n",
    "            try:\n",
    "                #print(a.find('a')['href'])\n",
    "                pages.append(a.find('a')['href'])\n",
    "            except:\n",
    "                pass\n",
    "        print()\n",
    "\n",
    "    return pages\n",
    "\n",
    "def get_allsides_pages(file) -> List[str]:\n",
    "    \"\"\"\n",
    "    Gets all known media pages from the AllSides website.\n",
    "    :return: The media pages to be scraped.\n",
    "    \"\"\"\n",
    "    pages: List[str] = []\n",
    "    print('# # # # # # # # # # # # # #')\n",
    "    print('Gathering pages in this category/site:')\n",
    "    print('https://allsides.com/')\n",
    "    print('THIS FILE HAS BEEN MANUALLY DOWNLOADED.')\n",
    "    bs = BeautifulSoup(open(file, 'r').read(), 'html.parser')\n",
    "    links = bs.find_all('td', attrs={'class', 'views-field views-field-title source-title'})\n",
    "    for a in links:\n",
    "        try:\n",
    "            #print(a.find('a')['href'])\n",
    "            pages.append(\"https://allsides.com\"+a.find('a')['href'])\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    return pages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_source(url: str) -> Source:\n",
    "    biaser = get_website_name(url)\n",
    "    rich_data = {}\n",
    "\n",
    "    try:\n",
    "        raw_html = simple_get(url)\n",
    "    except HTTPError as e:\n",
    "        raise print(f'The page \"{url}\" did not contain valid content.')\n",
    "    bs = BeautifulSoup(raw_html, 'html.parser')\n",
    "\n",
    "    if biaser == 'mediabiasfactcheck.com':\n",
    "        try:\n",
    "            source_name = bs.find('h1', attrs={'class', 'entry-title page-title'}).text.replace('\\n', '').replace('\\t', '')\n",
    "            if \".\" in source_name:\n",
    "                site_url = source_name\n",
    "                source_name = source_name.split('.')[0]\n",
    "            else:\n",
    "                site_url = \"N/A\"\n",
    "        except:\n",
    "            print(f'The page \"{url}\" does not have a name')\n",
    "            source_name = \"N/A\"\n",
    "            site_url = \"N/A\"\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            headings = bs.find_all('h1')\n",
    "            for heading in headings:\n",
    "                image = heading.find_next('img')\n",
    "            image_url: str = image[\"src\"]\n",
    "            image_url = image_url[:image_url.find('?')]\n",
    "        except Exception as e:\n",
    "            print(f'The source \"{source_name}\" with url \"{url}\" does not contain a left-right bias image.')\n",
    "            image_url = \"N/A\"\n",
    "            pass\n",
    "        \n",
    "        try:\n",
    "            rich_data['Bias Rating'] = bs.find('h2').find_next('span').text.split(' ')[0]\n",
    "        except: \n",
    "            pass\n",
    "        try:\n",
    "            body = bs.find('h3').find_next('p').text.replace(': ', ':').split('\\n')\n",
    "            for item in body:\n",
    "                rich_data[item.split(':')[0]] = item.split(':')[1]\n",
    "        except:\n",
    "            try:\n",
    "                body = bs.find('h3').find_next('h5').text.replace(': ', ':').split('\\n')\n",
    "                for item in body:\n",
    "                    rich_data[item.split(':')[0]] = item.split(':')[1]\n",
    "            except:\n",
    "                try:\n",
    "                    body = bs.find('h2').find_next('p').find_next('p').text.replace(': ', ':').split('\\n')\n",
    "                    for item in body:\n",
    "                        rich_data[item.split(':')[0]] = item.split(':')[1]\n",
    "                except:\n",
    "                    print(f'The source \"{source_name}\" with url \"{url}\" does not contain rich data or not in regular place.')\n",
    "                    print(f'Ommiting source after 3 format tries...')\n",
    "                    pass\n",
    "\n",
    "    \n",
    "    elif biaser == 'allsides.com':\n",
    "        try:\n",
    "            raw_html = simple_get(url)\n",
    "        except HTTPError as e:\n",
    "            raise print(f'The page \"{url}\" did not contain valid content.')\n",
    "        bs = BeautifulSoup(raw_html, 'html.parser')\n",
    "\n",
    "        try:\n",
    "            source_name = bs.find('h1').text.replace('\\n', '').replace('\\t', '')\n",
    "        except Exception as e:\n",
    "            print(f'The page \"{url}\" does not have a name')\n",
    "            source_name = \"N/A\"\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            image = bs.find('div', attrs={'class', 'news-source-full-area'}).find_next('img')\n",
    "            image_url: str = image[\"src\"]\n",
    "            image_url = image_url[:image_url.find('?')]\n",
    "        except Exception as e:\n",
    "            print(f'The source \"{source_name}\" with url \"{url}\" does not contain a left-right bias image.')\n",
    "            image_url = \"N/A\"\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            allsbias = bs.find('div',attrs={'class', 'source-page-bias-area source-page-bias-block'}).find_next('a').text.capitalize()\n",
    "            replacer = {'Lean left':'Left-Center', 'Lean right':'Right-Center'}\n",
    "            rich_data['Bias Rating'] = replacer.get(allsbias, allsbias)\n",
    "            site_url = bs.find('div',attrs={'class', 'span4'}).find_next('a')['href']\n",
    "            if \"http\" not in site_url:\n",
    "                site_url = \"N/A\"\n",
    "        except:\n",
    "            print(f'The source \"{source_name}\" with url \"{url}\" does not contain rich data or not in regular place.')\n",
    "            print(f'Lazy ommiting...')\n",
    "            site_url = \"N/A\"\n",
    "            pass\n",
    "\n",
    "    try:\n",
    "        country = rich_data['Country'].capitalize()\n",
    "    except:\n",
    "        country = \"N/A\"\n",
    "    try:\n",
    "        bias = rich_data['Bias Rating'].capitalize()\n",
    "        if rich_data['Bias Rating'] == 'Least': rich_data['Bias Rating'] = 'Center'\n",
    "    except:\n",
    "        bias = \"N/A\"\n",
    "    try:\n",
    "        factual = rich_data['Factual Reporting'].capitalize()\n",
    "    except:\n",
    "        factual = \"N/A\"\n",
    "    try:\n",
    "        press_freedom = rich_data['Press Freedom Rating'].capitalize()\n",
    "    except:\n",
    "        press_freedom = \"N/A\"\n",
    "    try:\n",
    "        media_type = rich_data['Media Type'].capitalize()\n",
    "    except:\n",
    "        media_type = \"N/A\"\n",
    "    try:\n",
    "        popularity = rich_data['Traffic/Popularity'].capitalize().split(' ')[0]\n",
    "    except:\n",
    "        popularity = \"N/A\"\n",
    "    try:\n",
    "        MBFC_credibility = rich_data['MBFC Credibility Rating'].capitalize().split(' ')[0]\n",
    "    except:\n",
    "        MBFC_credibility = \"N/A\"\n",
    "\n",
    "\n",
    "    print(\"--------------------------\")\n",
    "    print(\"Fully gathered:\", source_name)\n",
    "    print()\n",
    "    return Source(name=source_name, site_url=site_url, country=country, bias=bias, factual=factual, press_freedom=press_freedom, media_type=media_type, popularity=popularity, MBFC_credibility=MBFC_credibility, image_url=image_url, page_url=url, biaser=biaser)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_sources(urls: List[str]) -> Tuple[List[Source]]:\n",
    "    sources = []\n",
    "    for url in urls:\n",
    "            sources.append(scrape_source(url))\n",
    "    return sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources = ['https://mediabiasfactcheck.com/left/', 'https://mediabiasfactcheck.com/leftcenter/', 'https://mediabiasfactcheck.com/center/','https://mediabiasfactcheck.com/right-center/', 'https://mediabiasfactcheck.com/right/']\n",
    "\n",
    "fullPagesList = get_pages(sources) + get_allsides_pages('allsides.com.html')\n",
    "data = scrape_sources(fullPagesList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(data)\n",
    "data.to_csv('FullData.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
